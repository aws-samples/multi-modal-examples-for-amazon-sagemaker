{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e31cd0a-06a3-42b8-a5d3-4e07e2daaf18",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #04da79; border: 2px solid #039754; padding: 10px; color: black; font-family: Arial, Helvetica, sans-serif;\">\n",
    "    <strong>Kernel: Python 3 (ipykernel)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17db50c4-ad1f-446c-aafc-1a60793dd3c4",
   "metadata": {},
   "source": [
    "# Fine-tuning Qwen2-VL using the SWIFT framework on video/text dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a700c78-b5e2-456e-8b79-19b305a36f0e",
   "metadata": {},
   "source": [
    "We show how to fine-tune a vision Large Language Model (visionLLM) using the [SWIFT framework]((https://github.com/modelscope/ms-swift)) on a video-text to text dataset. \n",
    "The visionLLM can answer questions and extract information from the videos.\n",
    "\n",
    "A SageMaker remote function is being used to run the GPU intensive based training as a cost efficient SageMaker training job, where you only pay for the actual execution time and resources required by your job. \n",
    "\n",
    "## Why SWIFT for fine-tuning?\n",
    "[SWIFT](https://github.com/modelscope/ms-swift) (Scalable lightWeight Infrastructure for Fine-Tuning) is a comprehensive open-source framework designed to streamline the fine-tuning of large language models (LLMs) and multimodal large language models (MLLMs). \n",
    "As the most extensive fine-tuning framework available, SWIFT offers several key advantages:\n",
    "\n",
    "Key Features:\n",
    "- Supports over 350+ LLMs and 100+ MLLMs, making it the most comprehensive open-source framework for model fine-tuning\n",
    "- First training framework to provide systematic support for MLLMs\n",
    "- Integrates post-training processes including inference, evaluation, and model quantization\n",
    "- Unified interface for both text and multi-modal model training\n",
    "- Extensive dataset support with over 150 pure text and multi-modal datasets\n",
    "\n",
    "Technical Capabilities:\n",
    "- Compatible with multiple model architectures including Transformer and Mamba\n",
    "- Supports Megatron structured models for large-scale parallel pre-training\n",
    "- Implements state-of-the-art tuning techniques that can be used independently\n",
    "- Provides quantization options (BNB/GPTQ/AWQ) and LoRA merging capabilities\n",
    "- Enables deployment through PyTorch native and acceleration via vLLM and LMDeploy\n",
    "\n",
    "SWIFT represents a significant advancement in making LLM training more accessible and efficient, providing a complete technical pipeline that reduces the complexity of understanding and utilizing large models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a866d0ce-aa77-4bb2-a77f-7f68416c087a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc373ec-e955-4f14-b006-2cb0faaea764",
   "metadata": {},
   "source": [
    "Let's start by importing necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c216c9c0-c26e-4d4b-a809-f8cb352d2c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from IPython.display import JSON, Video\n",
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c04366b",
   "metadata": {},
   "source": [
    "## Dataset for Fine-Tuning\n",
    "\n",
    "We first need to setup the dataset that we want to do the finetuning on.\n",
    "\n",
    "* Download a small sample dataset based on [LLaVA-Video-178K](https://huggingface.co/datasets/lmms-lab/LLaVA-Video-178K) into your `./local_data` folder\n",
    "* Upload the dataset to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb7999e",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #efff94; border: 2px solid #efff94; padding: 10px; color: black; font-family: Arial, Helvetica, sans-serif;\">\n",
    "    Dataset <a href=\"https://huggingface.co/datasets/lmms-lab/LLaVA-Video-178K\">lmms-lab/LLaVA-Video-178K</a> is released to public under <a href=\"https://www.apache.org/licenses/LICENSE-2.0\">Apache 2.0 License</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564a95e2-ae8a-46c9-a8e9-93942c02a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "region = sagemaker.session.Session().boto_region_name\n",
    "session = sagemaker.Session()\n",
    "default_bucket_name = session.default_bucket()\n",
    "dataset_dir = \"./local_data\"\n",
    "dataset_s3_prefix = \"myvideotrainingdataset\"\n",
    "dataset_s3_uri = f\"s3://{default_bucket_name}/{dataset_s3_prefix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b679c01f-cd01-4fe5-9a69-551140de2756",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_path = snapshot_download(\n",
    "    repo_id=\"malterei/LLaVA-Video-small-swift\",\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=dataset_dir\n",
    ")\n",
    "print(f\"Downloaded dataset to local filepath: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea21a99-9a1c-4588-826f-14a72e61eb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_s3_uri = session.upload_data(\n",
    "    file_path, \n",
    "    bucket=default_bucket_name, \n",
    "    key_prefix=dataset_s3_prefix,\n",
    ")\n",
    "print(f\"Uploaded data from local: {file_path} to s3: {dataset_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947c501a",
   "metadata": {},
   "source": [
    "### View the dataset format\n",
    "\n",
    "The formatting of the dataset is the following\n",
    "\n",
    "* query: user query or request to be answered\n",
    "* response: expected response\n",
    "* videos: path to the video to be used in the context\n",
    "* history: previous conersational history. This can be a multi-turn user/assistant conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57611d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = os.path.join(\n",
    "    dataset_dir, \n",
    "    \"train.jsonl\"\n",
    ")\n",
    "df = pd.read_json(train_file, lines=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5310e07a",
   "metadata": {},
   "source": [
    "One row of this dataset described in the train.jsonl file looks as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6961a3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON(df.iloc[0].to_dict(), expanded=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de38d567",
   "metadata": {},
   "source": [
    "Lets look into one of the videos from this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc57065",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(os.path.join(dataset_dir, df.at[0,\"videos\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a6b52e",
   "metadata": {},
   "source": [
    "## Run fine-tuning as SageMaker training job\n",
    "\n",
    "Now we want to run the fine-tuning as a training job on SageMake using the remote decorator function.\n",
    "The remote decorator allows us to execute any python function very quickly as a remote SageMaker training job.\n",
    "The inputs and outputs will be automatically serialized/deserialized.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6d78be",
   "metadata": {},
   "source": [
    "Lets define the SageMaker distribution image to be used for us-east-1. \n",
    "The URI for other distributions and regions can be found in the [SageMaker Distribution documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-images.html#notebooks-available-images-arn).\n",
    "\n",
    "Here are a few example distributions from the link above:\n",
    "* **us-east-1:** 885854791233.dkr.ecr.us-east-1.amazonaws.com/sagemaker-distribution-prod:2.1.0-gpu\n",
    "* **us-west-2:** 542918446943.dkr.ecr.us-west-2.amazonaws.com/sagemaker-distribution-prod:2.1.0-gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d8332a-6bfc-4c6d-bce8-00cb7e691cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "from sagemaker.remote_function import remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c84688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets define the sagemaker distribution to use, we use 1.11 for now\n",
    "import sagemaker\n",
    "region = sagemaker.session.Session().boto_region_name\n",
    "\n",
    "if region==\"us-east-1\":\n",
    "    sagemaker_dist_uri = \"885854791233.dkr.ecr.us-east-1.amazonaws.com/sagemaker-distribution-prod:2.1.0-gpu\"\n",
    "elif region==\"us-west-2\":\n",
    "    sagemaker_dist_uri = \"542918446943.dkr.ecr.us-west-2.amazonaws.com/sagemaker-distribution-prod:2.1.0-gpu\"\n",
    "else:\n",
    "    assert False, \"Please make sure to manually set the `sagemaker_dist_uri` uri for your specific AWS region using the provided link from the cell above.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a4594e",
   "metadata": {},
   "source": [
    "By default, the [Amazon SageMaker Python SDK reads configuration](https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk) values from an admin defined or user specific configuration file. This configuration allows all kind of customizations do be made.\n",
    "Setting the `SAGEMAKER_USER_CONFIG_OVERRIDE` environment variable below overwrites these defaults.\n",
    "The main settings you will configure below are\n",
    "* The container image URI that should run the remote function code.\n",
    "* Python dependencies to install for the remote training.\n",
    "* Which files from the local working directory not to upload to the remote code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac98cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SAGEMAKER_USER_CONFIG_OVERRIDE\"] = os.getcwd() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017243d0-4813-4254-84b9-f2eb91b887bb",
   "metadata": {},
   "source": [
    "Define dependencies needed for fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38f03a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./requirements.txt\n",
    "\n",
    "git+https://github.com/huggingface/accelerate.git@v1.1.0\n",
    "ms-swift[llm]@git+https://github.com/modelscope/ms-swift.git@d13c431a0f337f6a04df2ce5310c5666b0c0c1f2\n",
    "git+https://github.com/huggingface/transformers@21fac7abba2a37fae86106f87fcf9974fd1e3830\n",
    "pyav\n",
    "qwen_vl_utils==0.0.8\n",
    "vllm>=0.6.1\n",
    "decord\n",
    "optimum\n",
    "qwen-vl-utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373a2e98-d6af-4694-bb43-89dd63496d19",
   "metadata": {},
   "source": [
    "Create a new config file to set the configuration for running remotely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da08fdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_yaml = f\"\"\"\n",
    "SchemaVersion: '1.0'\n",
    "SageMaker:\n",
    "  PythonSDK:\n",
    "    Modules:\n",
    "      RemoteFunction:\n",
    "        # role arn is not required if in SageMaker Notebook instance or SageMaker Studio\n",
    "        # Uncomment the following line and replace with the right execution role if in a local IDE\n",
    "        # RoleArn: <replace the role arn here>\n",
    "        S3RootUri: s3://{default_bucket_name}\n",
    "        ImageUri: {sagemaker_dist_uri}        \n",
    "        InstanceType: ml.g5.12xlarge\n",
    "        Dependencies: ./requirements.txt\n",
    "        IncludeLocalWorkDir: true\n",
    "        PreExecutionCommands:\n",
    "        - \"pip install packaging\"\n",
    "        CustomFileFilter:\n",
    "          IgnoreNamePatterns:\n",
    "          - \"local_data/*\"\n",
    "          - \"outputs/*\"\n",
    "          - \"docker-artifacts/*\"\n",
    "          - \"evaluation/*\"\n",
    "          - \"sample-media/*\"\n",
    "          - \"streamlit-ui/*\"\n",
    "          - \"accelerate/*\"\n",
    "          - \"container/*\"\n",
    "          - \"ms-swift/*\"\n",
    "          - \"model/*\"\n",
    "          - \"*.ipynb\"\n",
    "          - \"__pycache__\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(config_yaml, file=open('config.yaml', 'w'))\n",
    "print(config_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855ef6db-6268-4902-b185-cd396939e94a",
   "metadata": {},
   "source": [
    "The `@remote` decorator in Amazon SageMaker allows you to execute local Python functions as SageMaker training jobs with minimal code modifications. By annotating a function with @remote, SageMaker transforms the function's code into a training job, enabling it to run on managed infrastructure. This approach simplifies scaling and leverages SageMaker's capabilities without extensive changes to your existing codebase. \n",
    "\n",
    "- **Simplified Execution**: Wrap your local machine learning code with the @remote decorator to run it as a SageMaker training job, eliminating the need for manual job setup. \n",
    "\n",
    "- **Seamless Integration**: Continue developing in your preferred environment, such as Jupyter notebooks or IDEs, and use the decorator to offload computations to SageMaker's managed infrastructure. \n",
    "\n",
    "- **Flexible Configuration**: Customize training job parameters directly within the decorator or through a configuration file, specifying instance types, dependencies, and other settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf25821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@remote(\n",
    "    instance_type=\"ml.g5.12xlarge\", \n",
    "    volume_size=200, \n",
    "    use_spot_instances=False, \n",
    "    job_name_prefix=f\"multi-modal-finetune\", \n",
    "    keep_alive_period_in_seconds=300\n",
    ")\n",
    "def fine_tune_video(\n",
    "    training_data_s3, \n",
    "    train_data_path=\"train.jsonl\", \n",
    "    validation_data_path=\"validation.jsonl\"\n",
    "):\n",
    "    \n",
    "    from swift.llm.utils import SftArguments\n",
    "    from swift.llm.sft import llm_sft, get_sft_main\n",
    "\n",
    "    ## copy the training data from input source to local directory\n",
    "    dataset_dir = \".\"\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "    subprocess.run(['aws', 's3', 'cp', training_data_s3, dataset_dir, '--recursive'])\n",
    "    train_data_local_path = os.path.join(dataset_dir, train_data_path)\n",
    "    validation_data_local_path = os.path.join(dataset_dir, validation_data_path)\n",
    "\n",
    "    # set and run the fine-tuning using ms-swift framework\n",
    "    \n",
    "    sft_main = get_sft_main(SftArguments, llm_sft)\n",
    "    os.environ[\"NFRAMES\"]=json.dumps(24) # can be increased, but will require more memory\n",
    "    os.environ[\"MAX_PIXELS\"]=json.dumps(100352) # can be increased, but will require more memory\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\" # devices to be used\n",
    "    os.environ[\"NPROC_PER_NODE\"]=\"4\" # we have 4 GPUs on this instance\n",
    "    os.environ[\"USE_HF\"]=\"1\" # use huggingface\n",
    "    \n",
    "    argv = ['--model_type', 'qwen2-vl-2b-instruct',\n",
    "            '--model_id_or_path', 'Qwen/Qwen2-VL-2B-Instruct', \n",
    "            '--sft_type', 'lora', \n",
    "            '--output_dir', '/opt/ml/model' ,'--max_length', '2048',\n",
    "            '--dataset', train_data_local_path, \n",
    "            '--val_dataset', validation_data_local_path]\n",
    "    sft_main(argv)\n",
    "    return \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cffffd6-55af-492e-98ae-61abd863c4eb",
   "metadata": {},
   "source": [
    "That's all, now run your training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb119da2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"View your job here: https://{region}.console.aws.amazon.com/sagemaker/home?region={region}#/jobs/\")\n",
    "fine_tune_video(\n",
    "    dataset_s3_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370c35fe",
   "metadata": {},
   "source": [
    "Wait for the training job to complete. You can see the logs of the training above until it prints 'done' and the cell finishes executing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00b22d6-732e-4f07-922f-e6565e7fb390",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f5aeba; border: 2px solid #610010; padding: 10px; color: black; font-family: Arial, Helvetica, sans-serif;\">\n",
    "    TrainingJob on a <b>ml.g5.12xlarge</b> instance it should take <b>16-20 minutes</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559d782d",
   "metadata": {},
   "source": [
    "## Download Fine-Tuned Model for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73ec268-4155-46f8-bd77-330dc33aec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prefix = 'multi-modal-finetune-'\n",
    "models_list_s3 = !aws s3api list-objects-v2 --bucket {default_bucket_name} --prefix {base_prefix} --query \"Contents[?contains(Key, 'output/model.tar.gz')]|sort_by(@, &LastModified)[-1].Key\" --output text\n",
    "print(f\"found {models_list_s3[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cbf08e-e7d5-47f8-8ed1-44d51c8bebb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_suffix_s3 = models_list_s3[0]\n",
    "model_s3_path = os.path.join(\"s3://\", default_bucket_name, model_suffix_s3)\n",
    "print(f\"Fine-tuned Model Adapter: {model_s3_path}\")\n",
    "\n",
    "if not model_s3_path.endswith(\"model.tar.gz\"):\n",
    "    assert False, \"No latest fine-tuning found. Did your fine-tuning finish?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff20328-cd5b-49be-ab24-ad4a318fd574",
   "metadata": {},
   "source": [
    "We copy the model from S3 to our local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b92526",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_dir = \"./model\"\n",
    "model_destination = f\"{model_weights_dir}/{model_suffix_s3}\"\n",
    "model_dest_dir = str(Path(model_destination).parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d361485f-e54b-4ee9-94bb-a37bf913e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {model_s3_path} {model_destination}\n",
    "!tar --warning=no-unknown-keyword  -xzvf {model_destination} --directory {model_dest_dir} > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a0d7a6-4364-4f84-9f1f-3046a794bd81",
   "metadata": {},
   "source": [
    "Lets have a look what is inside of `model.tar.gz`:\n",
    "\n",
    "* The checkpoint directory contains the actual adapter\n",
    "* `adapter_model.safetensors` - contains the actual weights of the adapter\n",
    "\n",
    "For inference we can either use the adapter together with the original model, or we merge the adapter with the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b0124f-21ca-4069-aad2-38215da286cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd {model_dest_dir} && du -ah --max-depth=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6937aa12-f626-436d-9735-b03f22b3edfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_identifier = \"qwen2-vl-2b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd96400-0d48-4d67-8dc0-747caad87a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.join(model_dest_dir, model_identifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2a7f4d-b630-404c-bdc0-9c60df8615af",
   "metadata": {},
   "source": [
    "Swift outputs the directory that contains the weights of the best model. When training for a short time there might only be one checkpoint.\n",
    "If there are multiple checkpoints then the best checkpoint will be the model that performs the best on a specific metric. By default the metric is loss (reference `--predict_with_generate` in the [Swift documentation](https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html#sft-parameters)) and the model with the lowest evaluation loss on the validation dataset is considered the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502664db-eef3-495c-8492-036176693c78",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ffe347; border: 2px solid #610010; padding: 10px; color: black; font-family: Arial, Helvetica, sans-serif;\">\n",
    "    You will learn more about what evaluation loss means in the <b>Evaluation</b> chapter.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a117168b-4eea-4428-aae8-c09b7ee07194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helpers import find_latest_version_directory, find_best_model_checkpoint\n",
    "\n",
    "latest_version = find_latest_version_directory(model_dir)\n",
    "logging_file = os.path.join(os.getcwd(), model_dir, latest_version, \"logging.jsonl\")\n",
    "best_model_checkpoint = find_best_model_checkpoint(logging_file)\n",
    "if best_model_checkpoint:\n",
    "    best_model_checkpoint = best_model_checkpoint.replace(\"/opt/ml/model/\",\"\")\n",
    "    print(f\"best model checkpoint: {best_model_checkpoint}\")\n",
    "else:\n",
    "    print(\"Best model checkpoint not found. Please search the logs manually to find the path that stores the best model checkpoint.\")\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99dd8ef-de15-4098-82e8-95647a8f2a7b",
   "metadata": {},
   "source": [
    "## Evaluation of Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d2e5ce-462e-4933-97c5-9ccad955b86b",
   "metadata": {},
   "source": [
    "Let's look at the evaluation of a fine-tuning run. To save, you some time for the other labs we have already prepared the evaluation results and analysis for you below.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d3719e-d5c7-4464-9f48-687133d1cc22",
   "metadata": {},
   "source": [
    "### Explanation Evaluation Metrics from Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb710a5-d4be-4a04-86f8-f510e7b35ec0",
   "metadata": {},
   "source": [
    "When training a machine learning model it makes sense to split the dataset into a training dataset, validation dataset, and an optional test dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c7d737-b16d-4475-a8a1-f422d8bfa5d3",
   "metadata": {},
   "source": [
    "| Dataset Type | Purpose | Usage Phase | Characteristics | Model Interaction |\n",
    "|--------------|---------|-------------|-----------------|-------------------|\n",
    "| Training | Used to teach the model by adjusting its parameters and learning patterns. | Initial Training | Largest subset of data; must be representative of the overall dataset | Model actively learns and adjusts parameters based on this data |\n",
    "| Validation | Provides unbiased evaluation during training, prevent overfitting, helps fine-tune hyperparameters | During Training | Separate from training data; used for model optimization | Parameters can be adjusted based on performance, select between different model candidates |\n",
    "| Test | Offers final, unbiased evaluation of model performance | After Training | Completely unseen data; simulates real-world scenarios | No adjustments made; used only for final assessment |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13693882-9a91-49ea-ab80-194b1fac2316",
   "metadata": {},
   "source": [
    "Metrics that give insights into the performance of the training process are accuracy and loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bbf3a6-51d6-4ef5-bd1b-b1037414c1bf",
   "metadata": {},
   "source": [
    "#### Accuracy\n",
    "Accuracy measures how well your model performs on a dataset, expressed as a percentage of correct predictions. \n",
    "In this case the default accuracy used for fine-tuning is token level accuracy (parameter `--acc_strategy` in [swift documentation](https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html#sft-parameters)). [Token level accuracy is](https://github.com/modelscope/ms-swift/blob/552190740c1fefa836407a8dff53f5153bfbfa7e/swift/utils/metric.py#L53) defined as the number of correctly predicted tokens divided my the number of total predicted tokens. A token is the smallest unit that a language model generates. It is similar to words in the human vocabulary.\n",
    "\n",
    "Here is an example:\n",
    "| Metric | Dataset Label | Model Prediction |\n",
    "|--------|-------|------------|\n",
    "| Full Text | A. Hello World! | B. Hello Planet! |\n",
    "| Token IDs | [32, 13, 21927, 1879, 0] | [33, 13, 21927, 28835, 0] |\n",
    "\n",
    "_Matches:_ 3/5 tokens correct  \n",
    "_Accuracy:_ 60%\n",
    "\n",
    "For this sample the model output was [33, 13, 21927, 28835, 0] which decoded is \"B. Hello Planet!\". The label in the dataset is \"A. Hello World!\". So for this sample the accuracy is 60%.\n",
    "\n",
    "Accuracy helps you understand:\n",
    "* How well your model has learned the patterns in your data\n",
    "* Whether your model has sufficient capacity to learn the task\n",
    "* The overall progress of the learning process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a8c38b-f72f-4084-a719-a6b32d28aa76",
   "metadata": {},
   "source": [
    "#### Loss\n",
    "Loss is a more nuanced metric that quantifies the model’s prediction errors during training. It represents the “cost” or “penalty” the model incurs for making incorrect or imprecise predictions. Key aspects include:\n",
    "The loss function depends on the model architecture. [Qwen2 VL uses Cross Entropy](https://github.com/huggingface/transformers/blob/1a0cd69435cf6ddfef2b15e37cbddd94e13348f4/src/transformers/models/qwen2_vl/modeling_qwen2_vl.py#L30) as the loss function.\n",
    "\n",
    "Cross-entropy loss measures how well a language model predicts the next token in a sequence by comparing:\n",
    "* The model's predicted probability distribution over all possible next tokens\n",
    "* The actual token that appears in the dataset\n",
    "\n",
    "Loss calculation examples:\n",
    "* If model is very confident (90%) and correct: Small loss\n",
    "* If model is unsure (33%): Medium loss\n",
    "* If model is confident but wrong (5% for correct answer): Large loss\n",
    "\n",
    "Key Points\n",
    "* Loss is smallest when the model is confident and correct\n",
    "* Loss increases when the model:\n",
    "    * Is uncertain about the correct answer\n",
    "    * Is confident about the wrong answer\n",
    "* Lower loss values indicate better model performance\n",
    "* Perfect zero loss is impossible because language has natural ambiguity\n",
    "* It provides a more sensitive measure of model improvement than accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dff659e-1e66-4a67-90a5-6d79711679fe",
   "metadata": {},
   "source": [
    "### View Evaluation Metrics from actual fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bebd5e7-9429-4a3b-b34b-98deb8fb4d98",
   "metadata": {},
   "source": [
    "Now we apply the accuracy and loss metrics to the training dataset and validation (evaluation) dataset from an actual training run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98097aa-dc2d-479d-b741-1f64f3a2a44a",
   "metadata": {},
   "source": [
    "The swift library captures images of training metrics in the `images` directory of the model directory.\n",
    "Let's take a look at the training metrics of a training job with a [dataset of 5000 samples](https://huggingface.co/datasets/malterei/LLaVA-Video-large-swift). Fine-tuning Qwen2 VL 2B on the larger 5000 samples dataset took 5 hours and 39 minutes on a `ml.g5.48xlarge` instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff050b78-6858-4122-a848-e8022751dc82",
   "metadata": {},
   "source": [
    "#### Accuracy and loss on Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005964a0-d115-4701-b88a-0c0d7eba5405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "train_acc = Image(filename='evaluation/qwen2-vl-2b-instruct/large/v0-20241120-161510/images/train_acc.png') \n",
    "train_loss = Image(filename='evaluation/qwen2-vl-2b-instruct/large/v0-20241120-161510/images/train_loss.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fedf6d-1e81-488f-b0e6-c830c62d513d",
   "metadata": {},
   "source": [
    "Below is the training accuracy graph (**train/acc**). The model starts with relatively low **accuracy** (60%) and quickly improves within the first 25 epochs, reaching about 90% accuracy. After this initial sharp improvement, the accuracy stabilizes and fluctuates between 93-96% for the remainder of the training. This pattern demonstrates successful learning with diminishing returns over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bc6973-eb03-4aad-aed8-67e5ef1aa705",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56347c64-5d83-4115-b840-239cb1e066b7",
   "metadata": {},
   "source": [
    "The training loss (**train/loss**) graph below shows a classic exponential decay pattern that is typical of successful model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1afc98-c189-4317-a19a-da33e3f228b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77cbd89-0d55-4ab5-940a-3ffd3f9cf1d2",
   "metadata": {},
   "source": [
    "Together the training accuracy and training loss indicate that:\n",
    "* The model reaches a stable state without signs of overfitting.\n",
    "* The smooth curves suggest the learning rate was well-chosen for this task.\n",
    "* Improvements after the 200th epoch are minimal and early stopping would make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078f0d25-b82d-48a5-a356-51b0e606fa3c",
   "metadata": {},
   "source": [
    "#### Accuracy and loss on Validation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6149e95c-27a5-4912-9df7-bddd2f55107e",
   "metadata": {},
   "source": [
    "The evaluation metrics show the performance of the model on the validation dataset. \n",
    "\n",
    "The validation ensures that the model does not just memorize the training data but actually learns something about the task from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c674ab74-1ecf-476d-95bf-b52843a2cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_acc = Image(filename='evaluation/qwen2-vl-2b-instruct/large/v0-20241120-161510/images/eval_acc.png') \n",
    "eval_loss = Image(filename='evaluation/qwen2-vl-2b-instruct/large/v0-20241120-161510/images/eval_loss.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078886ad-6ff0-46c1-ae3f-133b1dc99655",
   "metadata": {},
   "source": [
    "Below are the graphs for the evaluation accuracy (**eval/acc**) and evaluation loss (**eval/loss**).\n",
    "Both evaluation metrics show consistent improvement without erratic behavior. This gradual improvement suggests good generalization to unseen real-world data.\n",
    "\n",
    "The evaluation accuracy demonstrates consistent improvement, starting at 96% around epoch 50 and gradually climbing to approximately 97% by epoch 300, with a slight peak around epoch 200. When compared to the training metrics, the relatively small gap between training and evaluation performance (training accuracy ~95% vs evaluation accuracy ~97%) suggests the model is not overfitting. \n",
    "\n",
    "Overfitting means the model would only learn the training data and not generalize well. In the metrics this would show if the training metrics keep improving while the evaluation metrics worsen or plateau.\n",
    "\n",
    "The evaluation loss shows a steady decrease from 0.125 to 0.095 over 300 epochs, with a notably smooth descent curve that indicates stable learning on unseen data. \n",
    "\n",
    "\n",
    "The relatively high accuracy and low loss values suggest this is a well-performing model on its intended multiple choice question answering task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a700a64-43ed-456e-8fd6-f4ea385bb5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(eval_acc)\n",
    "display(eval_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cb8bb8-1735-4adb-8316-88bb06795d9e",
   "metadata": {},
   "source": [
    "### Real-World Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1fff48-643a-463f-9f0d-47560de25e56",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ffe347; border: 2px solid #610010; padding: 10px; color: black; font-family: Arial, Helvetica, sans-serif;\">\n",
    "    You will not run inference for evaluation in this notebook. To save time and to optimize for maximal learning we already ran inference with different models for comparison. \n",
    "Please use the optional <a href=\"./04-02_optional_fine_tune_video_inference.ipynb\">notebook 04-02_optional_fine_tune_video_inference.ipynb</a> if you want to run inference on the small fine-tuned model yourself.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b328fa13-52d6-464c-bf5d-68e72f12cb1d",
   "metadata": {},
   "source": [
    "In addition to the training and evaluation metrics we have also run inference on an additional test dataset that has not been used during training.\n",
    "\n",
    "\n",
    "Large Language Models are challenging to evaluate due to their dynamic output generation, compared to simple classification tasks. The evaluation approach depends on the specific task and data, requiring data and task specific quantitative metrics and/or qualitative assessment methods for generated text, where multiple valid outputs may exist.\n",
    "\n",
    "Our dataset specifically is multiple choice questions on a video. In this case we have a correct multiple choice answer and can use that as a quantitative metric.\n",
    "\n",
    "Below you can see the accuracy (percentage of correct multiple choice answers) on the test dataset for the pre-trained model out of the box, the decent performance from fine-tuning on a small dataset, and the good performance when fine-tuning on a larger dataset for an extended time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e560175-ae09-47d4-9f74-c95f6d7b290b",
   "metadata": {},
   "source": [
    "| Model | Instance | Fine-Tuning Duration | Dataset Size | Accuracy |\n",
    "|----------|----------|----------|----------|----------|\n",
    "| Qwen2 VL 2B (vanilla)    | no fine-tuning     | no fine-tuning     | N/A | 50% |\n",
    "| Qwen2 VL 2B Fine-Tuned Small    | ml.g5.12xlarge     | 24 minutes     | 100 | 50% |\n",
    "| Qwen2 VL 2B Fine-Tuned Large    | ml.g5.48xlarge     | 5 hours 39 minutes     | 4660 | 83% |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d78844-6db2-4f55-9d41-4c33e863a677",
   "metadata": {},
   "source": [
    "We load the inference results from the three models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c5e5d4-9977-4352-b466-f3e4ffaeeb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen2_2b_responses = pd.read_json(\n",
    "    \"evaluation/qwen2-vl-2b-instruct/vanilla/outputs.jsonl\",\n",
    "    lines=True\n",
    ")\n",
    "qwen2_2b_fine_tuned_responses_small = pd.read_json(\n",
    "    \"evaluation/qwen2-vl-2b-instruct/small/outputs.jsonl\",\n",
    "    lines=True\n",
    ")\n",
    "qwen2_2b_fine_tuned_responses = pd.read_json(\n",
    "    \"evaluation/qwen2-vl-2b-instruct/large/outputs.jsonl\",\n",
    "    lines=True\n",
    ")\n",
    "evaluation_responses  = pd.merge(\n",
    "    qwen2_2b_responses, \n",
    "    qwen2_2b_fine_tuned_responses_small[['query','response', 'response_choice']],\n",
    "    on=\"query\",\n",
    "    suffixes=('', '_small_ft_qwen2_vl_2b')\n",
    ")\n",
    "evaluation_responses  = pd.merge(\n",
    "    evaluation_responses, \n",
    "    qwen2_2b_fine_tuned_responses[['query','response', 'response_choice']],\n",
    "    suffixes=('_vanilla_qwen2_vl_2b','_large_ft_qwen2_vl_2b'),\n",
    "    on=\"query\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9813cf29-b945-418b-a3d0-3a2d86ee704a",
   "metadata": {},
   "source": [
    "Below we render the input and outputs from the three different models:\n",
    "1. out of the box pre-trained vanilla Qwen2 VL 2B model\n",
    "2. fine-tuned Qwen2 VL 2B model on a small dataset\n",
    "3. fine-tuned Qwen2 VL 2B model on a large dataset\n",
    "\n",
    "We only show the rows in which one model made an incorrect predictions. Those predictions give us an idea what the model is getting wrong and how the three models are different. Fewer incorrect predictions is better.\n",
    "\n",
    "The first column in the table below contains the input prompt that is sent to the model along with the video in the second column. The *pre-trained response* column contains the prediction from Qwen2 VL 2B out of the box, the *small fine-tuned response* shows the prediction of a small 24 minutes fine-tuning and the *large fine-tuned response* of Qwen2 VL 2B fine-tuned on the larger dataset for 5 and a half hours. The last column contains the ground truth, so what we expect as the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224345c5-0fe9-48c1-a7d4-20d10e620d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "df = evaluation_responses.copy()\n",
    "\n",
    "incorrect = (df['label_choice'] != df['response_choice_vanilla_qwen2_vl_2b']) | (df['label_choice'] != df['response_choice_small_ft_qwen2_vl_2b']) | (df['label_choice'] != df['response_choice_large_ft_qwen2_vl_2b'])\n",
    "df = df[incorrect].reset_index(drop=True)\n",
    "\n",
    "def format_video(video_url):\n",
    "    if video_url:\n",
    "        return f'''<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"{os.path.join(dataset_dir,video_url)}\" type=\"video/mp4\">\n",
    "Your browser does not support the video tag.\n",
    "</video>\n",
    "'''\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def format_query_options(query_text):\n",
    "    # First split the question from options\n",
    "    query_text = query_text.replace(\"<video>\",\"\")\n",
    "    parts = query_text.split('A.')\n",
    "    if len(parts) < 2:\n",
    "        return query_text\n",
    "        \n",
    "    question = parts[0].strip()\n",
    "    options_text = 'A.' + parts[1]\n",
    "    \n",
    "    # Format options with line breaks\n",
    "    formatted_options = re.sub(\n",
    "        r'([A-D]\\. .*?)(?=[A-D]\\.|$)', \n",
    "        r'\\1\\n', \n",
    "        options_text\n",
    "    ).strip()\n",
    "    \n",
    "    return f\"{question}\\n{formatted_options}\"\n",
    "\n",
    "# Apply to dataframe\n",
    "df['videos'] = df['videos'].apply(format_video)\n",
    "df['query'] = df['query'].apply(format_query_options)\n",
    "\n",
    "\n",
    "# Select relevant columns for display\n",
    "display_columns = [\n",
    "    'query', \n",
    "    'videos',\n",
    "    'response_vanilla_qwen2_vl_2b', \n",
    "    'response_small_ft_qwen2_vl_2b',\n",
    "    'response_large_ft_qwen2_vl_2b',\n",
    "    'label'\n",
    "]\n",
    "\n",
    "def style_cell(is_correct):\n",
    "    color = '#c8e6c9' if is_correct else '#ffcdd2'  # Light green or light red\n",
    "    return f'background-color: {color}'\n",
    "\n",
    "def style_incorrect(column):\n",
    "    return [style_cell(df.loc[i, column.name.replace(\"response_\",\"response_choice_\")] == df.loc[i, 'label_choice'])\n",
    "        if re.match('^response_(?!choice)', column.name)\n",
    "        else ''\n",
    "        for i, _ in enumerate(column)\n",
    "    ]\n",
    "\n",
    "mapper = {\n",
    "    'label': 'Ground truth',\n",
    "    'videos': 'Video',\n",
    "    'query': 'Input prompt',\n",
    "    'response_vanilla_qwen2_vl_2b':'Pre-trained response', \n",
    "    'response_small_ft_qwen2_vl_2b':'Small fine-tuned response',\n",
    "    'response_large_ft_qwen2_vl_2b':'Large fine-tuned response'\n",
    "}\n",
    "\n",
    "styled_df = (df[display_columns].style\n",
    "    .apply(style_incorrect)\n",
    "    .set_properties(**{\n",
    "        'text-align': 'left',\n",
    "        'padding': '10px',\n",
    "        'border': '1px solid #ddd',\n",
    "        'white-space': 'pre-wrap',  # Add this line to preserve newlines\n",
    "        'height': 'auto',        # Allow height to adjust to content\n",
    "    })\n",
    "    .set_table_styles([\n",
    "        {'selector': 'table', 'props': [\n",
    "            ('table-layout', 'auto'),  # Allow different row heights\n",
    "            ('border-collapse', 'collapse')\n",
    "        ]},\n",
    "        {'selector': 'th', 'props': [\n",
    "            ('background-color', '#f5f5f5'),\n",
    "            ('text-align', 'left'),\n",
    "            ('padding', '10px'),\n",
    "            ('border', '1px solid #ddd')\n",
    "        ]}\n",
    "    ])\n",
    "    .set_properties(**{\n",
    "        'min-width': '220px'\n",
    "    }, subset=['query'])\n",
    "    .format_index(mapper.get, axis=1)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "display(styled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b9b716-7518-47eb-bd65-e7699baac7b3",
   "metadata": {},
   "source": [
    "The accuracy of the out of the box model and the small fine-tuned model is the same (50% accuracy). However, in the actual responses you can already see the small fine-tuning taught the model the expected output format of responding with the letter from the available choices (for example \"A.\"). The answer options that the fine-tuned model selects are still 50% incorrect. The hypothesis is that to understand the videos better we will need to fine-tune on more data.\n",
    "\n",
    "The *large fine-tuned response* column shows that the fine-tuned model on the larger dataset for longer resulted in only two incorrect predictions. We can see that now the model has learned not just the response format but also understanding videos it has never seen and answering multiple-choice questions with good success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83a85a5-7c63-404e-a473-50695f5bd74f",
   "metadata": {},
   "source": [
    "The evaluation showed us that fine-tuning helps to improve the performance of a vision large language model on our task.\n",
    "\n",
    "For your specific use case we also recommend running a qualitative human analysis of sample or failure cases, to ensure that the metric is aligning with your task specific expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11176312-e186-4194-aa5d-c2f1510ad50f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ffe347; border: 2px solid #610010; padding: 10px; color: black; font-family: Arial, Helvetica, sans-serif;\">\n",
    "    Please restart the notebook kernel to free up memory.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9342706b-2a49-4204-bc65-1bee99770e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "get_ipython().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843053cd-38d0-4a89-9dc0-44305c9a7e56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
